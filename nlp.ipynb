{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \" In the name of Allah, the Entirely meriful, the Especially Merciful. Praise is due to Allah, Loard of the worlds. The Entirely Merciful, the Especially Merciful. Sovereign of the day of recompense. It is You we worship and You we ask for help. Guide us to the straight path. The path of those You have bestowed favour, not of those who have evoked anger or of those who are astray.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' In the name of Allah, the Entirely meriful, the Especially Merciful.',\n",
       " 'Praise is due to Allah, Loard of the worlds.',\n",
       " 'The Entirely Merciful, the Especially Merciful.',\n",
       " 'Sovereign of the day of recompense.',\n",
       " 'It is You we worship and You we ask for help.',\n",
       " 'Guide us to the straight path.',\n",
       " 'The path of those You have bestowed favour, not of those who have evoked anger or of those who are astray.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' in the name of allah, the entirely meriful, the especially merciful. praise is due to allah, loard of the worlds. the entirely merciful, the especially merciful. sovereign of the day of recompense. it is you we worship and you we ask for help. guide us to the straight path. the path of those you have bestowed favour, not of those who have evoked anger or of those who are astray.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = word_tokenize(para.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'allah',\n",
       " 'entirely',\n",
       " 'meriful',\n",
       " 'especially',\n",
       " 'merciful',\n",
       " 'praise',\n",
       " 'due',\n",
       " 'allah',\n",
       " 'loard',\n",
       " 'worlds',\n",
       " 'entirely',\n",
       " 'merciful',\n",
       " 'especially',\n",
       " 'merciful',\n",
       " 'sovereign',\n",
       " 'day',\n",
       " 'recompense',\n",
       " 'worship',\n",
       " 'ask',\n",
       " 'help',\n",
       " 'guide',\n",
       " 'us',\n",
       " 'straight',\n",
       " 'path',\n",
       " 'path',\n",
       " 'bestowed',\n",
       " 'favour',\n",
       " 'evoked',\n",
       " 'anger',\n",
       " 'astray']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "token\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in token if word.casefold() not in stop_words]\n",
    "filtered_token = [word for word in filtered_tokens if word != '.']\n",
    "filtered_token1 = [word for word in filtered_token if word != ',']\n",
    "filtered_token1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_token = [stemmer.stem(word) for word in filtered_token1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'allah',\n",
       " 'entir',\n",
       " 'meri',\n",
       " 'especi',\n",
       " 'merci',\n",
       " 'prais',\n",
       " 'due',\n",
       " 'allah',\n",
       " 'loard',\n",
       " 'world',\n",
       " 'entir',\n",
       " 'merci',\n",
       " 'especi',\n",
       " 'merci',\n",
       " 'sovereign',\n",
       " 'day',\n",
       " 'recompens',\n",
       " 'worship',\n",
       " 'ask',\n",
       " 'help',\n",
       " 'guid',\n",
       " 'us',\n",
       " 'straight',\n",
       " 'path',\n",
       " 'path',\n",
       " 'bestow',\n",
       " 'favour',\n",
       " 'evok',\n",
       " 'anger',\n",
       " 'astray']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9781c75f8ebb8db568c34abc6fa56aff5b950f24b8cfd6fe8af6829c67f4ac57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
